# Python及大数据实验报告

### 实验环境

> - Python 3.9
> - intel core i5 8th, 16G

## 第一次实验

### Python安装

1. 从[python官网](https://www.python.org/)下载python3.9版本的安装包,按照默认路径安装，并配置环境变量

   ![image-20210328000433772](https://gitee.com/widealpha/pic/raw/master/image-20210328000433772.png)

2. 从jetbrains官网下载pycharm安装，并在pycharm中配置环境变量

   ![image-20210327234928414](https://gitee.com/widealpha/pic/raw/master/image-20210327234928414.png)

3. 在pycharm里进行编辑就可以了

### python实现归并排序

#### 介绍

**（归并排序放在了homework3文件夹中）**

merge_sort.py的main函数用以操作归并排序文件，文件必须以按行形式给出并且不能有不合规范的行

按照字母升序排列，输入文件放在./example/a.txt，结果输出至./result/sorted.txt

#### 分析

首先以main()函数开始

接着调用split_file()方法进行文件的拆分，并在拆分过程中进行一次排序，这次排序的时间复杂度为nlogn

接着调用merge()及进行归并，归并过程中时间复杂度为nlogn

最终时间复杂度为O(nlogn)

**实测速度很慢**，100000行的文件要将近1分钟，问题原因一部分是频繁的文件读写，另一部分是python自身较慢，两方面优化起来都很困难

现在已经采取文件缓冲的策略，每次读取10000(可以取更大的基数)行进行排序，速度有所提升

结论：python应该并不适合做大数据量的排序工作，更适合做一个胶水语言

### 四个小实验报告

1. 整除：直接利用python的整除运算符//即可轻松实现，考虑一下数字<100的情况就好了
2. 素数筛子：网上查阅素数筛子的资料，只要筛选到最后就可以获得素数，n<sup>2</sup>时间复杂度，但空间复杂度低
3. map：这个没有什么难度，要考虑键不存在的情况的判空
4. 循环：在上面的归并中有所讲解
5. 蒙蒂霍尔悖论游戏：更换的才中的概率是2/3，不换猜中的概率是1/3

## 第二次实验

### 正则去重

用split分开字符串，构造list并将删选变成set，最后输出set(注意顺序和加空格以及标点符号的问题)

### 函数调用

这个上面有很多体现，用函数调用可以减小程序耦合度，提高程序的可复用性

### 网络爬虫

> 最近做到一个项目，需要google的某一个字体，但是网络环境不太理想加载字体的速度比较缓慢。需要将字体下载到本地，但是下载到本地的字体文件有60M，作为一个web应用60m的数据实在是过于巨大，于是就看别人的项目的解决方案。
>
> 看到有一个项目用到了Google的差分字体，把大文件化整为零，感觉很好。
>
> [这是google的一个差分字体库](https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&family=Noto+Serif+JP:wght@400;700;900&display=swap)，于是参照格式对差分字体文件进行爬取

![image-20210402002055515](https://gitee.com/widealpha/pic/raw/master/image-20210402002055515.png)

> 首先字体文件的url有很强的规则性，比如以http://开头，以woff2结尾，所以可以用下面的正则筛选所有的链接记录

```python
urls_list = re.findall('https://.*\\.woff2', response.text)
```

> 首先遇到的第一个问题是，我在爬虫上直接用requests获取到的数据和在网页模式下获取的不一样，这时候就要添加User-Agent了
>
> ![因为不是浏览器所以没有下载浏览器的差分字体](https://gitee.com/widealpha/pic/raw/master/image-20210402105352367.png)

```python
headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                      ' (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36',
}
```

> 本来一开始爬的非常愉快，但是到了400个文件左右的时候速度骤减，分析了一波，是google限速了，只能换一个ip继续，又爬取了大约400个文件的时候，抛出了异常google服务器拒绝了连接
>
> 被google服务器反爬虫了
>
> 首先将代码块用try except包裹，用while True在外层保证执行，然后sleep函数控制休息时间，最后确定文件夹中文件的个数，以避免重复爬取。
>
> 这是最终爬取的结果
>
> ![image-20210402110150351](https://gitee.com/widealpha/pic/raw/master/image-20210402110150351.png)
>
> 一共爬取了2000个左右的文件，中间触发了一次google的人机身份验证

在爬取的时候，要注意处理服务器有无反爬虫的情况，如果有，要及时对反爬虫的信息进行处理，包括try except或者重连，或者更换ip等操作