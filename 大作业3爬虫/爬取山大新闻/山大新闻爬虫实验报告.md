# 山大新闻爬虫

## 准备环境

> 首先需要安装scrapy

```bash
pip install scrapy
```

## 使用

> 目前有两种使用方式
>
> 1. 直接调用当前目录下的main.py
>
> 2. 调用命令只进行爬取

### 调用main.py

运行main.py,会自动调用命令进行爬取，并保存结果

```python
os.system('scrapy crawl sdu_news -s LOG_FILE=spider.log -O news.json')
```

中间日志会保存到spider.log中,爬取结果会保存到news.json

利用json解析爬取到的数据，进行遍历搜索，提取详细信息

### 调用命令爬取

在当前目录下调用命令

```bash
scrapy crawl sdu_news -s LOG_FILE=spider.log -O news.json
```

进行爬取的结果存放在news.json中

### 运行结果

![新闻爬取全部详情](https://gitee.com/widealpha/pic/raw/master/image-20210418213925777.png)

![新闻爬取详情](https://gitee.com/widealpha/pic/raw/master/image-20210418213955670.png)

![新闻爬取详情](https://gitee.com/widealpha/pic/raw/master/image-20210418214010695.png)



## 编写记录

### ./sdu_new/items.py

items.py存放了爬取的产物的形式，并对内容进行了一定安全性保证

爬取的新闻的信息需要日期，标题等格式

```python
class SduNewsItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title = scrapy.Field()
    content = scrapy.Field()
    link = scrapy.Field()
    date = scrapy.Field()
    keywords = scrapy.Field()
```



### ./sdu_new/spider/news_spider.py

1. 初始化先要爬取的网址，这儿为了限制数据量没有用太多url

```python
 def start_requests(self):
        urls = [
            'https://www.view.sdu.edu.cn/xszh.htm',
            'https://www.view.sdu.edu.cn/xszh/201.htm',
            'https://www.view.sdu.edu.cn/xszh/200.htm',
            'https://www.view.sdu.edu.cn/xszh/199.htm',
            'https://www.view.sdu.edu.cn/xszh/198.htm',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)
```

2. 对爬取到的数据进行处理

    - 摘出标题 `item['title'] = response.css('title::text').get()`
     
    - 摘出标题关键字 `item['keywords'] += jieba.lcut_for_search(item['title'])`
     
    - 摘出内容 以及内容中的关键字信息
      
      ```python
      for sel in response.css('div.news_content p::text'):
          item['content'] += sel.extract()
         item['keywords'] += jieba.lcut_for_search(sel.extract())
      ```
      
    - 摘出日期以及处理日期的格式
    
      ```python
      item['date'] = response.css('div.news_tit p::text')
              if item['date']:
                  item['date'] = item['date'].get().replace('年', '-') \
                      .replace('月', '-').replace('发布日期：', '').replace('日', '').strip()
      ```
    
    - 对关键字列表去重 `item['keywords'] = list(set(item['keywords']))`
    
    - 追加新的url到url爬取列表中
    
      ```python
      for next_page in response.css('ul.sublist li a::attr(href)').getall():
           yield response.follow(next_page, callback=self.parse)
      ```

