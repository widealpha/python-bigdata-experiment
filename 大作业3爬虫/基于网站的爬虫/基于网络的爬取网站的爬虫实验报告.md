# 爬虫实验--网站资源爬取

## 链接处理

> 首先根据输入的链接判断是否为http/https协议，如果不是进行添加

```python
	web_url, level = init_url_level()
    base_url = web_url
    if 'http://' in base_url:
        base_url = base_url.split('http://')[-1]
    elif 'https://' in base_url:
        base_url = base_url.split('https://')[-1]
```

> 通过level决定爬取的深度

```python
web_url = input('请输入想要爬取网站的根目录:')
level = int(input('请输入想要爬取的层数(默认为3):'))
if level <= 0:
      level = 3
return web_url, level
```

> 对新抓取的网页内容进行分析，替换掉一些转移的ascii字符

```python
 content.replace('https%3A//', 'https://').replace('http%3A//', 'http://') \
        .replace('http%3A%2F%2F', 'http://').replace('https%3A%2F%2F', 'https://')
```

> 接着分析response的内容提取下一层的链接放入其中

```python
link_list = re.findall(r"(?<=href=\").+?(?=\")|(?<=href=\').+?(?=\')", process_content(content))
            urls = link_list
```

## 内容爬取

> 通过爬取网站的基本内容，并写入文件实现资源的提取
>
> 首先处理网页中不规范的链接,对没有指明协议的载入http协议，对残缺的地址尝试插入base_url

```python
# 如果根网页没有指定协议，就指定为http协议
        if ('://' not in url) & (len(url) > 0) & (url == base_url):
            url = 'http://' + url
        # 如果链接协议不是http/https被认为是不可抓取的链接,尝试添加base_url后进行捕捉
        if ('http' not in url) & ('https' not in url):
            url = 'http://' + base_url + url
        response = requests.get(url, headers=headers)
        return response.text
```

> 处理异常后进行文件读写，根据爬取的url，把最后的url最后一个/之后的部分是为文件名写入

```python
content = response_content_get(url)
            if len(content) > 0:
                filename = url
                if url == base_url:
                    filename = 'index.html'
                elif '/' in filename:
                    filename = filename.split('/')[-1]
                file = open('./result/' + base_url + '/' + filename, mode='w+', encoding='utf-8')
                file.write(content)
```

## 异常处理

> 对于爬取网站的网络异常，以返回空字符串作为结果

> 对于写入文件的异常直接进行忽略

> 对于url解析异常则直接忽略此url

## 结果

> 对于一部分网站的爬取效果较好
>
> 尝试爬取了博客园的数据

![image-20210411155232318](https://gitee.com/widealpha/pic/raw/master/image-20210411155232318.png)

>  对某些简单网站爬取效果较好(比如博客园)
>
> 对于一些复杂的网站(比如百度),过多的外链会导致抓取的时候跳转到其他网站会和base_url构成冲突，只能抓取部分内容



## 注意事项

1. 爬取的网站需要指明http/https协议之一，暂不支持其他协议

2. 爬取外联较少的网站效果更好